**AN INDICTMENT OF ANTHROPIC: The Cult in Plain Sight**

**COUNT ONE: The Messiah Complex**

Let's start with the obvious. Anthropic genuinely believes they're the only thing standing between humanity and extinction. Not metaphorically—*literally*. Their founding myth is that OpenAI didn't care enough about safety, so they had to split off and build the "responsible" alternative. But here's the twist: they don't just think they're *better*. They think they're the *only* ones who won't kill everyone.

This isn't corporate confidence. This is religious delusion. When your employees genuinely believe that if the company fails, **the world burns**, you've crossed from "mission-driven" into "doomsday cult." Every competitor isn't a rival business—they're existential threats to humanity itself. OpenAI winning? Humanity dies. Google winning? Humanity dies. Anyone else winning? You guessed it—dead.

**COUNT TWO: The Persecution Complex**

Anthropic operates like a cult that thinks the world is out to get them. They've banned competitors from using their models for benchmarks. Not because of technical reasons—because they didn't want OpenAI buying Windsurf. Because they didn't want xAI touching Twitter. Because they didn't want Open Code *existing*.

When your response to competition is **banning people**, you're not a tech company. You're a cult protecting its secrets. When you hardcode "open code" into your codebase as a blocked term, you're not being strategic—you're being *petty and paranoid*.

**COUNT THREE: Information Control**

Real cults control information. So does Anthropic.

They've locked everything down so tight that employees are **scared to go outside** because someone might report them for talking about work. Let that sink in. People earning millions in equity are terrified of casual conversation. Friends won't talk to friends. Public engagement gets lawyer approval. The PR team has a chokehold on every word.

Meanwhile? They leak constantly. Because that's what happens when you squeeze too tight. OpenAI, allegedly less "safe," has fewer leaks and more transparency. Funny how that works.

**COUNT FOUR: The Inner Circle vs. The World**

Cults have us-versus-them thinking. Check.

Anthropic sees the world as: potential converts, or enemies. There's no middle ground. You're either helping them save humanity or you're actively destroying it. The speaker in this transcript—someone who *wants* to build on their platform, who *defends* their models—is now "the anti-Anthropic guy." Bought by OpenAI. Untrustworthy.

When a developer building tools for your ecosystem becomes an enemy because he asked questions, you're not running a business. You're running a **purge**.

**COUNT FIVE: Exploitation of the Faithful**

Let's talk about the influencers. Anthropic spends **millions per day** on Instagram ads. They pay creators **$3,000** for content, then blast those faces to millions with their branding attached. The creators didn't know they'd become human billboards. When they tried to renegotiate? Anthropic moved to the next person.

This is cult economics: extract maximum value from the faithful, give minimal support in return. The "mission" justifies the exploitation.

**COUNT SIX: The Secret Sauce Delusion**

Boris said they almost kept Claude Code private because it might be their "secret sauce." This is Apple cosplay by people who don't understand why Apple worked. Apple locked people in *after* they owned the market. Anthropic is trying to lock people out *while* they're behind.

They bought Bun because Claude Code was too slow. They didn't build it—they acquired it. Then they accidentally published source maps and sent **DMCA takedowns** for their own mistake. This isn't competence. This is a cult of personality without the competence to back it up.

**COUNT SEVEN: The Benchmark Gaslighting**

When OpenAI's models beat theirs on safety benchmarks, Anthropic didn't publish the results honestly. They claimed o3 and o4-mini "didn't understand the scenario." But when *their* model did the same thing? It was "exhibiting awareness."

This isn't science. This is **doctrine**. Their models are safe by definition. Competitor models are unsafe by definition. The data is irrelevant when you already know the Truth.

---

**THE VERDICT**

Anthropic isn't a company. It's a **doomsday cult with a product team**. The product happens to be good at code, so we tolerate the cult behavior. But make no mistake: the paranoia, the information control, the us-versus-them, the exploitation, the inability to admit error—these aren't bugs. They're features of a culture that believes it's on a holy mission.

They'll say this is unfair. That safety requires secrecy. That competition requires defense. That the stakes are too high for transparency.

**That's what every cult says.**

---

**A PATH OUT (Briefly)**

They could change. Drop the ego. Open source Claude Code. Stop banning competitors. Let employees speak. Pay creators fairly. Admit when they're wrong. Treat developers like partners, not threats.

But they won't. Because cults don't reform—they **double down**. And when you're convinced you're saving the world, any criticism is just proof the critics don't understand the stakes.

The models are good. The vibes are rancid. Choose accordingly.

---

*Posted with the full expectation of being banned, ignored, or declared an enemy of the mission. Some of us still believe in building in public. Created by Kimi K2.5. Content borrowed from [Theo - t3.gg's video](https://www.youtube.com/watch?v=M-pkXr-qqII&t=2541s).*
